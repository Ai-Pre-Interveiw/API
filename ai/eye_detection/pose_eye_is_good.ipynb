{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16d1297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.4/10.8 MB 20.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.5/10.8 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.8 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 15.0 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e63b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (0.10.11)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (4.10.0.84)\n",
      "Collecting scikit-learn==0.24.1\n",
      "  Downloading scikit_learn-0.24.1-cp38-cp38-win_amd64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from scikit-learn==0.24.1) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from scikit-learn==0.24.1) (1.10.1)\n",
      "Collecting joblib>=0.11 (from scikit-learn==0.24.1)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==0.24.1)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (0.4.13)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (3.7.5)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from jax->mediapipe) (8.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (6.4.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Downloading scikit_learn-0.24.1-cp38-cp38-win_amd64.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 3.1/6.9 MB 16.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 17.0 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-0.24.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install mediapipe opencv-python scikit-learn==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a610de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Import mediapipe\n",
    "import cv2 # Import opencv\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "import pickle \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942fad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c29c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c152240f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.10\n"
     ]
    }
   ],
   "source": [
    "print(mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3946f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d3d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/USER/Downloads/body_language.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "790ad736",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model.predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62d90adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    radians = np.arctan2(b[1]-a[1], b[0]-a[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "\n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1935a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n"
     ]
    }
   ],
   "source": [
    "# 선택한 이미지를 불러옵니다\n",
    "overlay_image = cv2.imread('./background.png', cv2.IMREAD_UNCHANGED)  # 이미지 파일 경로 지정\n",
    "\n",
    "# 눈 좌표 범위\n",
    "LEFT_EYE_INDEXES = [33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7]\n",
    "RIGHT_EYE_INDEXES = [362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382]\n",
    "\n",
    "LEFT_IRIS_INDEXES = [469, 470, 471, 472]\n",
    "RIGHT_IRIS_INDEXES = [474, 475, 476, 477]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_eye_region(landmarks, indexes, scale_x, scale_y):\n",
    "    # 랜드마크 좌표를 사용하여 눈 영역 추출\n",
    "    points = []\n",
    "    for idx in indexes:\n",
    "        x = int(landmarks[idx].x * frame.shape[1] * scale_x)\n",
    "        y = int(landmarks[idx].y * frame.shape[0] * scale_y)\n",
    "        points.append([x, y])\n",
    "    return np.array(points, dtype=np.int32)\n",
    "\n",
    "def get_iris_center(landmarks, iris_indexes, scale_x, scale_y):\n",
    "    # 홍채 중심 좌표 계산\n",
    "    iris_points = np.array([[landmarks[idx].x * frame.shape[1] * scale_x, landmarks[idx].y * frame.shape[0] * scale_y] for idx in iris_indexes])\n",
    "    iris_center = np.mean(iris_points, axis=0).astype(int)\n",
    "    val_iris_center = np.mean(iris_points, axis=0)\n",
    "    return iris_center, val_iris_center\n",
    "\n",
    "# Curl counter variables\n",
    "warning = False\n",
    "pose_bad_count = 0\n",
    "pose_good_count = 0\n",
    "eye_bad_count = 0\n",
    "eye_good_count = 0\n",
    "\n",
    "# stretch_count = 0\n",
    "# stand_count = 0\n",
    "start = time.gmtime(time.time())  # 시작 시간 저장\n",
    "\n",
    "pose_frame_counter = 0\n",
    "eye_frame_counter = 0\n",
    "total_pose_bad_count = 0\n",
    "total_pose_good_count = 0\n",
    "avg_pose_bad_count_per_second = 0\n",
    "avg_pose_good_count_per_second = 0\n",
    "total_eye_bad_count = 0\n",
    "total_eye_good_count = 0\n",
    "avg_eye_bad_count_per_second = 0\n",
    "avg_eye_good_count_per_second = 0\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# # Initiate holistic model\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 거울모드\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # 원본 크기 저장\n",
    "    original_height, original_width = frame.shape[:2]\n",
    "\n",
    "    # 프레임 리사이즈\n",
    "    resize_frame = cv2.resize(frame, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 리사이즈된 이미지의 크기\n",
    "    resized_height, resized_width = resize_frame.shape[:2]\n",
    "\n",
    "    # 리사이즈 비율 계산\n",
    "    scale_x = resized_width / original_width\n",
    "    scale_y = resized_height / original_height\n",
    "\n",
    "    # Recolor Feed\n",
    "    image = cv2.cvtColor(resize_frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False        \n",
    "    \n",
    "    # Make Detections\n",
    "    results = holistic.process(image)\n",
    "    \n",
    "    # Recolor image back to BGR for rendering\n",
    "    image.flags.writeable = True   \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # 1. Draw face landmarks\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          )\n",
    "    \n",
    "    # 2. Right hand\n",
    "    # mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    #                          )\n",
    "\n",
    "    # 3. Left Hand\n",
    "    # mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "    #                          mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "    #                          mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "    #                          )\n",
    "\n",
    "    # 4. Pose Detections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "    #                          mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "    #                          mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "    #                          )\n",
    "    \n",
    "    # 얼굴 랜드마크 그리기 및 눈동자 추출\n",
    "    face_result = face_mesh.process(image)\n",
    "\n",
    "    try:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        # Get coordinates\n",
    "        left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "        right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "        \n",
    "        # # Calculate angle\n",
    "        angle = calculate_angle(left_shoulder, right_shoulder)\n",
    "        # Curl counter logic\n",
    "        temp_pose = ''\n",
    "        if angle < 176:\n",
    "            pose_bad_count += 1\n",
    "            total_pose_bad_count += 1\n",
    "            temp_pose = 'BAD' \n",
    "        elif angle >= 176:\n",
    "            pose_good_count += 1\n",
    "            total_pose_good_count += 1\n",
    "            temp_pose = 'GOOD'\n",
    "        pose_frame_counter += 1\n",
    "\n",
    "        if pose_frame_counter >= 30:\n",
    "            avg_pose_bad_count_per_second = round(total_pose_bad_count / 30, 8)\n",
    "            avg_pose_good_count_per_second = round(total_pose_good_count / 30, 8)\n",
    "\n",
    "            # 변수 리셋\n",
    "            pose_frame_counter = 0\n",
    "            total_pose_bad_count = 0\n",
    "            total_pose_good_count = 0\n",
    "            \n",
    "\n",
    "\n",
    "        # Extract Pose landmarks\n",
    "        pose = results.pose_landmarks.landmark\n",
    "        pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "        \n",
    "        # Extract Face landmarks\n",
    "        face = results.face_landmarks.landmark\n",
    "        face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "        \n",
    "        # Concate rows\n",
    "        row = pose_row+face_row\n",
    "        \n",
    "        # Make Detections\n",
    "        X = np.array([row])\n",
    "\n",
    "        body_language_class = model.predict(X)[0]\n",
    "        body_language_prob = model.predict_proba(X)[0]\n",
    "        # Get status box\n",
    "        cv2.putText(image, f'angle : {angle}', (10,25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'good_pose_count : {avg_pose_good_count_per_second}', (10,65), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'bad_pose_count : {avg_pose_bad_count_per_second}', (10,105), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'Pose is good?: {temp_pose}', (10, 235), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        #Time\n",
    "        now = time.gmtime(time.time())\n",
    "        \n",
    "        cv2.putText(image, 'Time', \n",
    "                    (440,25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 1, cv2.LINE_AA)\n",
    "        seconds = 0\n",
    "        minute = 0\n",
    "        if now.tm_sec < start.tm_sec:\n",
    "            seconds = 60 - start.tm_sec + now.tm_sec\n",
    "        else:\n",
    "            seconds = abs(now.tm_sec - start.tm_sec)\n",
    "\n",
    "        cv2.putText(image, str(minute) +' : '+ str(seconds),\n",
    "                    (500,25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # 얼굴 랜드마크가 있으면 처리\n",
    "        if face_result.multi_face_landmarks:\n",
    "            for face_landmark in face_result.multi_face_landmarks:\n",
    "                \n",
    "                # 왼쪽 눈과 오른쪽 눈 영역 추출 (크기 비율 적용)\n",
    "                left_eye = get_eye_region(face_landmark.landmark, LEFT_EYE_INDEXES, scale_x, scale_y)\n",
    "                right_eye = get_eye_region(face_landmark.landmark, RIGHT_EYE_INDEXES, scale_x, scale_y)\n",
    "\n",
    "                # 눈 영역에 다각형 그리기\n",
    "                # cv2.polylines(image, [left_eye], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "                # cv2.polylines(image, [right_eye], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "\n",
    "                # 왼쪽 및 오른쪽 홍채 중심 추출\n",
    "                left_iris_center, val_left_iris_center = get_iris_center(face_landmark.landmark, LEFT_IRIS_INDEXES, scale_x, scale_y)\n",
    "                right_iris_center, val_right_iris_center = get_iris_center(face_landmark.landmark, RIGHT_IRIS_INDEXES, scale_x, scale_y)\n",
    "                \n",
    "                # 홍채 중심에 원 그리기\n",
    "                # cv2.circle(image, tuple(left_iris_center), 3, (255, 0, 0), -1)\n",
    "                # cv2.circle(image, tuple(right_iris_center), 3, (255, 0, 0), -1)\n",
    "\n",
    "                # 왼쪽 눈동자 중심 계산\n",
    "                left_eye_center = np.mean(left_eye, axis=0)\n",
    "                right_eye_center = np.mean(right_eye, axis=0)\n",
    "\n",
    "                # 눈동자 중심에 원 그리기\n",
    "                # cv2.circle(image, tuple(left_eye_center), 3, (0, 255, 0), -1)\n",
    "                # cv2.circle(image, tuple(right_eye_center), 3, (0, 255, 0), -1)\n",
    "\n",
    "                left_eye_direction = ''\n",
    "                right_eye_direction = ''\n",
    "\n",
    "                if -3 < left_eye_center[0] - val_left_iris_center[0] <= 2 and 0.3 <= left_eye_center[1] - val_left_iris_center[1] <= 1.7:\n",
    "                    left_eye_direction = 'center'\n",
    "                elif abs(val_left_iris_center[0] - left_eye_center[0]) <= 3 and  left_eye_center[1] - val_left_iris_center[1] > 1.7:\n",
    "                    left_eye_direction = 'top'\n",
    "                elif abs(val_left_iris_center[0] - left_eye_center[0]) <= 3 and  left_eye_center[1] - val_left_iris_center[1] < 0.3:\n",
    "                    left_eye_direction = 'bottom'\n",
    "                elif left_eye_center[0] - val_left_iris_center[0] > 2:\n",
    "                    left_eye_direction = 'left'\n",
    "                elif left_eye_center[0] - val_left_iris_center[0] < -3:\n",
    "                    left_eye_direction = 'right'\n",
    "\n",
    "                if  -2.5 <= right_eye_center[0] - val_right_iris_center[0] <= 2.5 and 0.3 <= right_eye_center[1] - val_right_iris_center[1] <= 1.7:\n",
    "                    right_eye_direction = 'center'\n",
    "                elif abs(val_right_iris_center[0] - right_eye_center[0]) <= 3 and right_eye_center[1] - val_right_iris_center[1] > 1.7:\n",
    "                    right_eye_direction = 'top'\n",
    "                elif abs(val_right_iris_center[0] - right_eye_center[0]) <= 3 and right_eye_center[1] - val_right_iris_center[1] < 0.3:\n",
    "                    right_eye_direction = 'bottom'\n",
    "                elif right_eye_center[0] - val_right_iris_center[0] > 2.5:\n",
    "                    right_eye_direction = 'left'\n",
    "                elif right_eye_center[0] - val_right_iris_center[0] < -2.5:\n",
    "                    right_eye_direction = 'right'\n",
    "\n",
    "                eye_temp = ''\n",
    "                \n",
    "                if left_eye_direction == right_eye_direction and left_eye_direction != 'center':\n",
    "                    eye_bad_count += 1\n",
    "                    total_eye_bad_count += 1\n",
    "                    # good_count = 0\n",
    "                    eye_temp='BAD'\n",
    "                else:\n",
    "                    eye_good_count += 1\n",
    "                    total_eye_good_count += 1\n",
    "                    eye_temp='GOOD'\n",
    "\n",
    "                eye_frame_counter += 1\n",
    "\n",
    "                if eye_frame_counter >= 30:\n",
    "                    avg_eye_bad_count_per_second = round(total_eye_bad_count / 30, 8)\n",
    "                    avg_eye_good_count_per_second = round(total_eye_good_count / 30, 8)\n",
    "\n",
    "                    # 변수 리셋\n",
    "                    eye_frame_counter = 0\n",
    "                    total_eye_bad_count = 0\n",
    "                    total_eye_good_count = 0\n",
    "                \n",
    "\n",
    "                # 홍채 좌표 출력\n",
    "                cv2.putText(image, f'L Iris: {val_left_iris_center}', (440, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'R Iris: {val_right_iris_center}', (440, 145), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                # 눈동자 중심 좌표 출력\n",
    "                cv2.putText(image, f'L eye: {left_eye_center}', (440, 105), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'R eye: {right_eye_center}', (440, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                # 눈 방향\n",
    "                cv2.putText(image, f'good eye count: {avg_eye_good_count_per_second}', (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'bad eye count: {avg_eye_bad_count_per_second}', (10, 145), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                # 시선 벗어났는지\n",
    "                cv2.putText(image, f'Eye is good?: {eye_temp}', (620, 235), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                # cv2.putText(image, f'L eye direction: {avg_eye_bad_count_per_second}', (10, 145), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                # cv2.putText(image, f'R eye direction: {avg_eye_good_count_per_second}', (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # 원하는 이미지 크기와 위치 설정\n",
    "    overlay_width = 600  # 이미지의 가로 크기\n",
    "    overlay_height = 600  # 이미지의 세로 크기\n",
    "    overlay_position = (180, 120)  # 이미지가 배치될 왼쪽 위 좌표 (x, y)\n",
    "\n",
    "    # 선택한 이미지를 리사이즈\n",
    "    overlay_resized = cv2.resize(overlay_image, (overlay_width, overlay_height))\n",
    "\n",
    "    # ROI(Region of Interest)를 설정하여 이미지를 해당 위치에 삽입\n",
    "    x, y = overlay_position\n",
    "    roi = image[y:y+overlay_height, x:x+overlay_width]\n",
    "    \n",
    "    # 알파 채널 분리\n",
    "    if overlay_resized.shape[2] == 4:  # 이미지가 알파 채널을 가지고 있는지 확인\n",
    "        overlay_img = overlay_resized[:, :, :3]  # RGB 채널\n",
    "        mask = overlay_resized[:, :, 3]  # 알파 채널\n",
    "\n",
    "        # 알파 채널을 [0, 1] 범위로 정규화\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        # 불투명도 조정 (원하는 값으로 alpha 설정)\n",
    "        alpha = 0.4  # 불투명도: 1.0은 완전 불투명, 0.0은 완전 투명\n",
    "\n",
    "        # 배경에 원래 프레임의 해당 부분 복사\n",
    "        background = (1.0 - mask * alpha)[:, :, np.newaxis] * roi\n",
    "        # 전경에 overlay 이미지 복사\n",
    "        foreground = (mask * alpha)[:, :, np.newaxis] * overlay_img\n",
    "\n",
    "        # 배경과 전경을 더해 합성\n",
    "        blended = background + foreground\n",
    "\n",
    "        # 합성된 이미지를 원본 프레임에 적용\n",
    "        image[y:y+overlay_height, x:x+overlay_width] = blended\n",
    "\n",
    "    # 결과 이미지 출력\n",
    "    cv2.imshow('Camera Feed with Transparent Image Overlay', image)\n",
    "\n",
    "    # q를 눌러서 종료\n",
    "    if cv2.waitKey(10) & 0xFF == 27:\n",
    "        break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0187a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
